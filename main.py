import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn import tree
from graphviz import render

if len(sys.argv) == 3:
  train = sys.argv[1] == "train"
  k = int(sys.argv[2])
elif len(sys.argv) == 2:
  train = sys.argv[1] == "train"
  k = 5
else:
  train = True
  k = 5

# Read in data
data = pd.read_csv("api_calls.csv")

# Use One Hot Encoding for categorical features
cat_features = [column for column in data.columns if column != "hash" and column != "malware"]
data = pd.get_dummies(data, columns=cat_features)

# Perform random undersampling of majority class (malware)
goodware_samples = data[data.malware == 0]
malware_samples = data[data.malware == 1]
undersampled_malware = malware_samples.sample(len(goodware_samples), axis=0, replace=False)
undersampled_data = pd.concat([goodware_samples, undersampled_malware], axis=0)

# Separate data into features and labels
X = undersampled_data.drop(["hash", "malware"], axis=1)
y = undersampled_data["malware"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Create a decision tree to classify data
clf = DecisionTreeClassifier(random_state=42)

# Decision Tree Classifier found using grid search
best_clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=30)

# If we're training, then do k-fold cross-validation using training data
if train:
    params = {'splitter': ('best', 'random'),
              'criterion': ('gini', 'entropy'),
              'max_depth': (1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, None)}

    grid = GridSearchCV(clf, params, scoring="f1", verbose=1, n_jobs=-1, cv=k)
    grid.fit(X_train, y_train)

    print(f"Best params: {grid.best_params_}")
    print(f"Best F1 score: {grid.best_score_}")

else:
    # Fit decision tree and make predictions
    best_clf.fit(X_train, y_train)
    y_pred = best_clf.predict(X_test)

    with open("classifications.txt", "w") as fp:
        y_pred_index = 0
        for i in y_test.index:
            fp.write(f"{data.at[i, 'hash']}, {y_pred[y_pred_index]}, {y_test.loc[i]}\n")
            y_pred_index += 1

    with open("results.txt", "w") as fp:
        fp.write(f"AUC ROC Score : {roc_auc_score(y_test, y_pred)}\n")
        fp.write(f"F1 Score  : {f1_score(y_test, y_pred)}\n")
        fp.write(f"Precision : {precision_score(y_test, y_pred)}\n")
        fp.write(f"Recall    : {recall_score(y_test, y_pred)}\n")
        fp.write(f"Accuracy  : {accuracy_score(y_test, y_pred)}\n")

    dotfile = open("tree_visualization.dot", "w")
    tree.export_graphviz(best_clf, out_file=dotfile, feature_names=X.columns)
    dotfile.close()

    render(engine="dot", filepath="tree_visualization.dot", format="png")