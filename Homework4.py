import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn import tree

# Whether to do k-fold cross-validation for training or to make predictions
train = 0
k = 5

# Read in data
data = pd.read_csv("api_calls.csv")

# Use One Hot Encoding for categorical features
cat_features = list(data.drop(["hash", "malware"], axis=1))
data = pd.get_dummies(data, columns=cat_features)

# Perform random undersampling of majority class (malware)
goodware_samples = data[data.malware == 0]
malware_samples = data[data.malware == 1]
undersampled_malware_indices = np.random.choice(malware_samples.index, size=len(goodware_samples), replace=False)
undersampled_data = pd.concat([goodware_samples, malware_samples.loc[undersampled_malware_indices]])

# Separate data into features and labels
X = undersampled_data.drop(["hash", "malware"], axis=1)
y = undersampled_data["malware"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a decision tree to classify data
clf = DecisionTreeClassifier(random_state=42)

# Decision Tree Classifier found using grid search
best_clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=40)

# If we're training, then do k-fold cross-validation using training data
if train:
    params = {'splitter': ('best', 'random'), 'criterion': ('gini', 'entropy'), 'max_depth': (1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, None)}
    grid = GridSearchCV(clf, params, scoring="f1")
    
    grid.fit(X_train, y_train)
    print("Highest F1 Score:", grid.best_score_)
    print("Best Model:", grid.best_estimator_)
    
    print(cross_val_score(grid.best_estimator_, X_train, y_train, cv=k, scoring='f1', n_jobs=-1))


else:
    # Fit decision tree and make predictions
    best_clf.fit(X_train, y_train)
    y_pred = best_clf.predict(X_test)
    
    with open("classification.txt", "w") as fp:
        y_pred_index = 0
        for i in y_test.index:
            fp.write("{}  {}  {}\n".format(data.at[i, 'hash'], y_pred[y_pred_index], y_test.loc[i]))
            y_pred_index += 1
    
    with open("results.txt", "w") as fp:
        fp.write("F1 Score  : {}\n".format(f1_score(y_test, y_pred)))
        fp.write("Precision : {}\n".format(precision_score(y_test, y_pred)))
        fp.write("Recall    : {}\n".format(recall_score(y_test, y_pred)))
        fp.write("Accuracy  : {}".format(accuracy_score(y_test, y_pred)))
    
    dotfile = open("tree_visualization.dot", "w")
    tree.export_graphviz(best_clf, out_file=dotfile, feature_names=X.columns)
    dotfile.close()